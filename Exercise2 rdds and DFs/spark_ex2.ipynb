{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exercise 1\n",
      "without partitions\n",
      "--- 16.295982122421265 seconds ---\n",
      "done\n",
      "Exercise 1\n",
      "with 5 partitions\n",
      "--- 15.735426187515259 seconds ---\n",
      "done\n",
      "Exercise 2\n",
      "done\n",
      "Exercise 3\n",
      "90164\n",
      "done\n",
      "Exercise 4\n",
      "[('Robin Bovey', 239.0), ('Walter Gretzky', 239.0), ('Chris C. Fisher', 239.0), ('Charles Gill', 228.0), ('Neil R Carlson', 228.0), ('Alison Bell', 228.0), ('Ed Smith', 228.0), ('Ronald Comer', 228.0), (\"Old Farmer's Almanac\", 228.0), ('Carlton Fredericks', 228.0), ('James W. Kalat', 228.0), ('V. Wayne Klemin', 228.0), ('Robert H. Ramsey', 228.0), ('Mark Pfetzer', 219.0), ('Christian R. Hirsch', 204.0), ('Rink Van Der Velde', 204.0), ('Don Gerrard', 204.0), ('Alan Neibauer', 204.0), ('Gerard Egan', 201.0), ('Adrian Goldsworthy', 201.0)]\n",
      "done\n",
      "Exercise 5\n",
      "90164\n",
      "90164\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Student Name :\n",
    "Student ID   :\n",
    "\"\"\"\n",
    "from __future__ import print_function\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from book import Book\n",
    "from user import User\n",
    "from bookrating import BookRating\n",
    "import shutil\n",
    "import os, errno\n",
    "import time\n",
    "\n",
    "def silentremove(filedir): #helper function to clear rdd folders and their content\n",
    "    try:\n",
    "        shutil.rmtree(filedir)\n",
    "    except FileNotFoundError: \n",
    "        pass\n",
    "    \n",
    "\n",
    "class ExerciseSet2(object):\n",
    "    \"\"\"\n",
    "    Big Data Frameworks Exercises\n",
    "    https://www.cs.helsinki.fi/courses/582740/2017/k/k/1\n",
    "    \"\"\"\n",
    "        \n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initializing Spark Conf and Spark Context here\n",
    "        Some Global variables can also be initialized\n",
    "        \"\"\"\n",
    "        self.conf = (SparkConf().setMaster(\"local\").\n",
    "                     setAppName(\"exercise_set_2\").\n",
    "                     set(\"spark.executor.memory\", \"2g\"))\n",
    "        self.spark_context = SparkContext(conf=self.conf)\n",
    "        \n",
    "        # Have global variables here if you wish\n",
    "        # self.global_variable = None\n",
    "\n",
    "    def exercise_1(self,partitions=False):\n",
    "        \"\"\"\n",
    "        INTRSUCTIONS: Download the Book-Crossing data set in CSV format from:\n",
    "        https://moodle.helsinki.fi/pluginfile.php/915616/mod_forum/attachment/1239670/BX-dataset-fixe\n",
    "        d.tar.gz\n",
    "        Read users, books and ratings as RDDs using the textFile function. You should use separate\n",
    "        serializable case classes for each csv files. The members of the classes are the columns of the\n",
    "        csv files. Finally you will have three RDDs of three different object types and save these object\n",
    "        RDDs (Hint: use booksrdd .saveAsObjectFile for Scala, and use booksrdd. saveAsPickleFile\n",
    "        for Python).\n",
    "        ● BX-Books has isbn, title, author, year, publisher, url1, url2, url3 (Hint: Inside map\n",
    "        function skip these urls. Your case class for books should have isbn, title, author, year,\n",
    "        publisher fields.)\n",
    "        ● BX-Book-Ratings has user_id, isbn, rating (Your case class for ratings should have\n",
    "        user_id, isbn, rating fields.)\n",
    "        ● BX-Users has user_id, location, age (Your case class for users should have user_id,\n",
    "        location, age fields.)\n",
    "        You should use two partition settings; without specifying partitions, and specifying 5 partitions\n",
    "        inside the textFile function and discuss the performance gain in total execution time.\n",
    "        \"\"\"\n",
    "        print(\"Exercise 1\")\n",
    "        \n",
    "        # remove previous instances of saved RDD objects in case they exist\n",
    "        silentremove(\"users\")\n",
    "        silentremove(\"books\")\n",
    "        silentremove(\"ratings\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        sc = self.spark_context #for convenience abbreviate spark context variable\n",
    "        \n",
    "        if partitions: # use 5 partitions\n",
    "            print('with 5 partitions')\n",
    "            u = sc.textFile(\"BX-Users.csv\", 5) #reads csv file\n",
    "            b = sc.textFile(\"BX-Books.csv\", 5)\n",
    "            r = sc.textFile(\"BX-Book-Ratings.csv\", 5)\n",
    "        else:\n",
    "            print('without partitions')\n",
    "            u = sc.textFile(\"BX-Users.csv\") #reads csv file\n",
    "            b = sc.textFile(\"BX-Books.csv\")\n",
    "            r = sc.textFile(\"BX-Book-Ratings.csv\")\n",
    "        \n",
    "        #clean csv file\n",
    "        umap = u.map( lambda line : line.replace(\"\\\"\", \"\").split(\";\")) # takes out new line and quotes characters \\\" out \n",
    "        # from csv format and splits the data with semicolon ; separator\n",
    "        bmap = b.map( lambda line : line.replace(\"\\\"\", \"\").split(\";\"))\n",
    "        rmap = r.map( lambda line : line.replace(\"\\\"\", \"\").split(\";\"))\n",
    "        \n",
    "        usersRDD = umap.map(lambda x : User(x[0],x[1],x[2])) # map items from clean csv file to their corresponding class fields\n",
    "        booksRDD = bmap.map(lambda x : Book(x[0],x[1],x[2],x[3],x[4]))\n",
    "        ratingsRDD = rmap.map(lambda x : BookRating(x[0],x[1],x[2]))\n",
    "        \n",
    "        # save new rdd objects\n",
    "        usersRDD.saveAsPickleFile(\"users\")\n",
    "        booksRDD.saveAsPickleFile(\"books\")\n",
    "        ratingsRDD.saveAsPickleFile(\"ratings\")\n",
    "        print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "        print('done')\n",
    "        return None\n",
    "\n",
    "    def exercise_2(self):\n",
    "        \"\"\"\n",
    "        Read users, books and ratings object RDDs from the disk, as you saved in the first exercise.\n",
    "        Join all the RDDs so that your result is only one RDD where each element is a book with its\n",
    "        corresponding reviews. The final structure could be something like RDD[(ISBN, title, author,\n",
    "        year, publisher, Set(userID, location, age, rating))].\n",
    "        Hint: Reading ObjectFile or PickleFile can be done using the SparkContext object.\n",
    "        Be sure to choose correct data types for each field, e.g. ratings are Integers. You can use case\n",
    "        classes to help your work. Bear in mind that although the value for age is an integer, some\n",
    "        entries have a “NULL” value which cannot be parsed with the toInt() method. Use whatever\n",
    "        placeholder value you deem appropriate instead, like -1.\n",
    "        Hint: Remember that you can rearrange fields within an RDD using map() and you can use the\n",
    "        join() method to merge elements of the form (key, value) which have the same key.\n",
    "        \"\"\"\n",
    "        sc = self.spark_context\n",
    "        silentremove(\"rated_books\")\n",
    "        \n",
    "        print(\"Exercise 2\")\n",
    "        \n",
    "        #load rdds from saved pickle files\n",
    "        usersRDD = sc.pickleFile(\"users\")\n",
    "        booksRDD = sc.pickleFile(\"books\")\n",
    "        ratingsRDD = sc.pickleFile(\"ratings\")\n",
    "        \n",
    "        \n",
    "        uratings = usersRDD.map(lambda u : (u.user_id,(u.location,u.age))).join(ratingsRDD.map(lambda r : (r.user_id,(r.isbn,r.rating)))) \n",
    "        # map usersRDD with key,value tuple e.g. (u.user_id,(u.location,u.age))) where key=u.user_id and value=(u.location,u.age)\n",
    "        #Then, map ratingsRDD with key,value tuple where key is also user_id, so that they can be joined with join transformation\n",
    "        \n",
    "        bookmap = booksRDD.map(lambda b: (b.isbn,(b.title,b.author,b.year,b.publisher))) # map booksRD with ISBN as key\n",
    "        \n",
    "        uratingsmap = uratings.map(lambda x : (x[1][1][0],(x[0],x[1][0][0],x[1][0][1],x[1][1][1]))) # map uratings with ISBN \n",
    "        #as key, where we now use the tuple indeces instead of the class names\n",
    "        bookjoin = bookmap.join(uratingsmap) #join both key-value RDDs by isbn key\n",
    "        #print(bookjoin.take(1)) #display the content of one RDD\n",
    "        bookjoin.saveAsPickleFile(\"rated_books\")\n",
    "        print('done')\n",
    "        return None\n",
    "\n",
    "    def exercise_3(self):\n",
    "        \"\"\"\n",
    "        Write a function that computes how many reviews there are for books published between two\n",
    "        given years. Apply the function to the resulting RDD from exercise 2 and find out the number of\n",
    "        reviews for books published between 1992 and 1998.\n",
    "        Hint: You can use filter() to omit elements with other publication dates.\n",
    "        \"\"\"\n",
    "        \n",
    "        def reviewsCount(obj,startyear,endyear):\n",
    "            yearfilt = obj.filter(lambda x : x[1][0][2] <= endyear and x[1][0][2] >= startyear)\n",
    "            reviewscount = yearfilt.groupByKey().count()\n",
    "            return reviewscount\n",
    "        \n",
    "        sc = self.spark_context\n",
    "        \n",
    "        print(\"Exercise 3\")\n",
    "        \n",
    "        #load rdds from saved pickle files\n",
    "        rated_books = sc.pickleFile(\"rated_books\")\n",
    "        reviewscount = reviewsCount(rated_books,1992,1998)\n",
    "        print(reviewscount)\n",
    "        print(\"done\")\n",
    "        return None\n",
    "\n",
    "    def exercise_4(self):\n",
    "        \"\"\"\n",
    "        Write a function that takes the RDD created in Exercise 1, and returns an RDD of the 20 authors\n",
    "        with the highest average age among their reviewers. Filter out the entries with a “NULL” value.\n",
    "        Use only operations over RDDs (count, sort, etc.); the use of for loops is not allowed.\n",
    "        \"\"\"\n",
    "        print(\"Exercise 4\")\n",
    "        \n",
    "        sc = self.spark_context\n",
    "        usersRDD = sc.pickleFile(\"users\")\n",
    "        booksRDD = sc.pickleFile(\"books\")\n",
    "        ratingsRDD = sc.pickleFile(\"ratings\")   \n",
    "        \n",
    "        uratings = usersRDD.map(lambda u : (u.user_id,u.age)).join(ratingsRDD.map(lambda r : (r.user_id,(r.isbn,r.rating))))         \n",
    "        bookmap = booksRDD.map(lambda b: (b.isbn,b.author)) # map booksRD as ISBN, author\n",
    "        \n",
    "        uratingsmap = uratings.map(lambda x : (x[1][1][0],x[1][0])) # map uratings as ISBN, rater's age\n",
    "        authorsjoin = bookmap.join(uratingsmap)\n",
    "        authorsmap = authorsjoin.map(lambda x : x[1]).filter(lambda a : a[1] > -1)\n",
    "        \n",
    "        zval = (0,0)\n",
    "        aggregate_authors = authorsmap.aggregateByKey(zval, lambda a,b: (a[0] + b,    a[1] + 1), lambda a,b: (a[0] + b[0], a[1] + b[1]))\n",
    "        # average with aggregate method. zval is our initial values tuple. first value will be the sum accumulator and the second value the counter accumulator\n",
    "        # first lambda is w.r.t. a -> our initial tuple zval, and b -> escalar, next value from another k,v pair with same key\n",
    "        #second lambda is w.r.t. a -> current tuple of zval, and b -> following tuple of zval with same key as the current\n",
    "        #therefore in first lambda we increase our sum accumulator with the initial age=0 and the next age, and the initial counter=0 with +1\n",
    "        #then in second lambda we add the current accumulator tuple with the next accumulator tuple\n",
    "        # See https://stackoverflow.com/questions/29930110/calculating-the-averages-for-each-key-in-a-pairwise-k-v-rdd-in-spark-with-pyth\n",
    "        \n",
    "        average_authors = aggregate_authors.mapValues(lambda v: v[0]/v[1]) #then we calculate the average v[0] is the resulting accumulator sum\n",
    "        #and v[1] is the resulting accumulator counter\n",
    "        top_authors = average_authors.takeOrdered(20,key=lambda x : -x[1]) #finally we use takeOrdered to sort. negative sign is for descending order \n",
    "        #and 20 is to take the top 20 values\n",
    "        \n",
    "        print(top_authors) #print results\n",
    "        print(\"done\")\n",
    "            \n",
    "        return None\n",
    "\n",
    "    def exercise_5(self):\n",
    "        \"\"\"\n",
    "        Repeat exercise one with dataframes. You will save three DataFrames for three textfiles. You\n",
    "        do not need to construct the case class objects. Apply filtering to find out the number of reviews\n",
    "        for books published between 1992 and 1998.\n",
    "        \"\"\"\n",
    "        \n",
    "        print(\"Exercise 5\")\n",
    "        \n",
    "        # remove previous instances of saved RDD objects in case they exist\n",
    "        silentremove(\"users\")\n",
    "        silentremove(\"books\")\n",
    "        silentremove(\"ratings\")\n",
    "        \n",
    "        #remove previous DFs\n",
    "        silentremove(\"usersDF\")\n",
    "        silentremove(\"booksDF\")\n",
    "        silentremove(\"ratingsDF\")\n",
    "        \n",
    "        sc = self.spark_context #for convenience abbreviate spark context variable\n",
    "        sqlContext = SQLContext(sc) #SQL context to create dataframes and be able to perform SQL operations\n",
    "        \n",
    "        #load data as dataframes\n",
    "        u = sqlContext.read.csv('BX-Users.csv',sep=\";\")\n",
    "        b = sqlContext.read.csv('BX-Books.csv',sep=\";\")\n",
    "        r = sqlContext.read.csv('BX-Book-Ratings.csv',sep=\";\")\n",
    "        \n",
    "        #convert existing dataframes to new dataframes with specified column names\n",
    "        uDF = u.toDF(\"user_id\", \"location\", \"age\")\n",
    "        bDF = b.toDF(\"isbn\", \"title\",\"author\",\"year\",\"publisher\",\"url1\",\"url2\",\"url3\")\n",
    "        rDF = r.toDF(\"user_id\", \"isbn\", \"rating\")\n",
    "        \n",
    "        #drop unused columns\n",
    "        bDF = bDF.drop(\"url1\",\"url2\",\"url3\")\n",
    "        \n",
    "        #cast specific columns to IntegerType\n",
    "        uDF = uDF.withColumn(\"age\",uDF[\"age\"].cast(IntegerType()))\n",
    "        bDF = bDF.withColumn(\"year\",bDF[\"year\"].cast(IntegerType()))\n",
    "        rDF = rDF.withColumn(\"rating\",rDF[\"rating\"].cast(IntegerType()))\n",
    "        \n",
    "        uDF.write.save(\"usersDF\")\n",
    "        bDF.write.save(\"booksDF\")\n",
    "        rDF.write.save(\"ratingsDF\")\n",
    "        \n",
    "        #replace None with -1 for integer type data\n",
    "        uDF = uDF.na.fill({\"age\" : -1})\n",
    "        bDF = bDF.na.fill({\"year\" : -1})\n",
    "        rDF = rDF.na.fill({\"rating\" : -1})\n",
    "        \n",
    "        rbDF = bDF.join(rDF,\"isbn\") #join books DF and ratings DF by isbn column\n",
    "        \n",
    "        #option a) filter rated books with dataframe operations\n",
    "        filtered_books = rbDF.filter(\"year >= 1992 and year <= 1998\").groupby(\"isbn\").agg({\"isbn\" : \"count\"})\n",
    "        print(filtered_books.count())\n",
    "        \n",
    "        #option b) filter rated books with SQL operations\n",
    "        rbDF.registerTempTable(\"rbtable\") #save as temporary table for SQL queries\n",
    "        sqlDF = sqlContext.sql(\"\"\"SELECT isbn, COUNT(*) FROM rbtable WHERE year >= 1992 AND year <= 1998 GROUP BY isbn\"\"\")\n",
    "        print(sqlDF.count())\n",
    "        \n",
    "        return None\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    EXERCISESET2 = ExerciseSet2()\n",
    "    EXERCISESET2.exercise_1()\n",
    "    EXERCISESET2.exercise_1(True)\n",
    "    EXERCISESET2.exercise_2()\n",
    "    EXERCISESET2.exercise_3()\n",
    "    EXERCISESET2.exercise_4()\n",
    "    EXERCISESET2.exercise_5()\n",
    "    EXERCISESET2.spark_context.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
